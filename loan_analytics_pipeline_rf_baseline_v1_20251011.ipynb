{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b7a8ef2-a818-407c-97c1-33718e4cf0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loan_model_pipeline.py\n",
    "# Run as a script or in notebook cells. Requires: pandas, sqlalchemy, pyodbc, scikit-learn, joblib\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from sqlalchemy import create_engine, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4aebb92d-7348-460e-b658-b2c59fe4ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('loan_analytics_dev',)]\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Connection config\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "SERVER = \"localhost\"  # your default instance\n",
    "DATABASE = \"loan_analytics_dev\"\n",
    "\n",
    "# Build connection string\n",
    "odbc_str = f\"DRIVER={{{DRIVER}}};SERVER={SERVER};DATABASE={DATABASE};Trusted_Connection=yes;\"\n",
    "conn_str = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(odbc_str)}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(conn_str, fast_executemany=True)\n",
    "\n",
    "# Test query (no tables required!)\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT DB_NAME() AS ConnectedTo;\"))\n",
    "    print(result.fetchall())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bd9c60e-646e-4aa4-bfbc-a8bb5ba2f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vw_CleanLoans ...\n",
      "Rows loaded: 1000\n",
      "   ApplicationID  CustomerID ApplicationDate  Age   Income  EmploymentYears  \\\n",
      "0           2001        1494      2023-04-20   54  2205134               10   \n",
      "1           2002        1124      2024-09-18   33  2955165                8   \n",
      "2           2003        1333      2023-05-19   30  1592245                7   \n",
      "3           2004        1380      2024-10-05   38  2528538               19   \n",
      "4           2005        1406      2024-02-09   31  2129240               23   \n",
      "\n",
      "   LoanAmount  LoanTermMonths    Purpose  LoanToIncome  MissedPaymentsCount  \\\n",
      "0      499177              48   Personal        0.2264                    1   \n",
      "1      101295              24       Home        0.0343                    1   \n",
      "2      950519              24  Education        0.5970                    1   \n",
      "3     1381666              48       Home        0.5464                    1   \n",
      "4      657564              36  Education        0.3088                    1   \n",
      "\n",
      "  LastPaymentDate  \n",
      "0      2023-05-19  \n",
      "1      2025-07-15  \n",
      "2      2023-06-17  \n",
      "3      2025-01-31  \n",
      "4      2024-09-03  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: load vw_CleanLoans\n",
    "sql = \"SELECT * FROM dbo.vw_CleanLoans;\"\n",
    "print(\"Loading vw_CleanLoans ...\")\n",
    "df = pd.read_sql(sql, engine)\n",
    "print(\"Rows loaded:\", len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab1bd382-8cd9-481f-ace1-caa3e0e67e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Basic cleaning & target\n",
    "# -------------------------\n",
    "# Working target: loan is 'delinquent' if MissedPaymentsCount >= 1\n",
    "# This is a business proxy — adjust as needed.\n",
    "df['target_default'] = (df['MissedPaymentsCount'].fillna(0) >= 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0a4329-3150-43a0-ade5-32622a4b40f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAM\\AppData\\Local\\Temp\\ipykernel_21084\\234430527.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = pd.to_datetime(datetime.utcnow().date())\n"
     ]
    }
   ],
   "source": [
    "# Feature ideas: Age, Income, EmploymentYears, LoanAmount, LoanTermMonths, LoanToIncome, MissedPaymentsCount, DaysSinceLastPayment\n",
    "# Create DaysSinceLastPayment (large number if NULL)\n",
    "today = pd.to_datetime(datetime.utcnow().date())\n",
    "df['LastPaymentDate'] = pd.to_datetime(df['LastPaymentDate'])\n",
    "df['DaysSinceLastPayment'] = (today - df['LastPaymentDate']).dt.days.fillna(9999).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df93d25f-eb1c-46a5-b192-0ed5de8e64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a handful of features (you can expand)\n",
    "features = [\n",
    "    'Age',\n",
    "    'Income',\n",
    "    'EmploymentYears',\n",
    "    'LoanAmount',\n",
    "    'LoanTermMonths',\n",
    "    'LoanToIncome',\n",
    "    'MissedPaymentsCount',   # useful but correlated with target in our proxy\n",
    "    'DaysSinceLastPayment',\n",
    "    'Purpose'                # categorical\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f298fc1c-d70c-4eca-971f-e600df5ca835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dataset rows: 1000\n"
     ]
    }
   ],
   "source": [
    "# Select usable rows (drop weird nulls)\n",
    "df_model = df[features + ['ApplicationID', 'CustomerID', 'ApplicationDate', 'target_default']].copy()\n",
    "df_model = df_model.dropna(subset=['ApplicationID', 'CustomerID', 'ApplicationDate'])\n",
    "print(\"Model dataset rows:\", len(df_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "336d0389-dbb9-420d-8723-9a2087f9dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train/test split\n",
    "# -------------------------\n",
    "X = df_model[features]\n",
    "y = df_model['target_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d0a57d8-9a9b-4db0-9b07-055f25bf4743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 800 Test rows: 200\n"
     ]
    }
   ],
   "source": [
    "# Simple train-test split (stratify to keep class ratio)\n",
    "X_train, X_test, y_train, y_test, meta_train, meta_test = train_test_split(\n",
    "    X, y, df_model[['ApplicationID', 'CustomerID', 'ApplicationDate']],\n",
    "    test_size=0.2, random_state=42, stratify=y if y.nunique() > 1 else None\n",
    ")\n",
    "print(\"Train rows:\", len(X_train), \"Test rows:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1dc3c73-5374-42dc-97d6-eefab7e8d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor output shape (sample): (5, 12)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Preprocessing pipeline (robust to sklearn versions)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import inspect\n",
    "\n",
    "# features\n",
    "num_features = ['Age','Income','EmploymentYears','LoanAmount','LoanTermMonths','LoanToIncome','MissedPaymentsCount','DaysSinceLastPayment']\n",
    "cat_features = ['Purpose']\n",
    "\n",
    "# numeric transformer\n",
    "num_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='median')\n",
    ")\n",
    "\n",
    "# detect which kwarg OneHotEncoder accepts ('sparse' or 'sparse_output')\n",
    "ohe_kwargs = {}\n",
    "sig = inspect.signature(OneHotEncoder)\n",
    "if 'sparse_output' in sig.parameters:\n",
    "    ohe_kwargs['sparse_output'] = False\n",
    "else:\n",
    "    # older sklearn\n",
    "    ohe_kwargs['sparse'] = False\n",
    "\n",
    "# categorical transformer\n",
    "cat_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='Missing'),\n",
    "    OneHotEncoder(handle_unknown='ignore', **ohe_kwargs)\n",
    ")\n",
    "\n",
    "# ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_features),\n",
    "        ('cat', cat_transformer, cat_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Quick smoke-check (optional): fit_transform on a small slice to ensure shape is OK\n",
    "# Only run this if X_train exists; otherwise skip\n",
    "try:\n",
    "    sample = X_train.head(5)\n",
    "    ft = preprocessor.fit_transform(sample)\n",
    "    print(\"Preprocessor output shape (sample):\", ft.shape)\n",
    "except NameError:\n",
    "    print(\"X_train not found — re-run the data-prep cells before running this check.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a50b148-27cb-464d-a9ab-d9e98763de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest pipeline ...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Train RandomForest\n",
    "# -------------------------\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\n",
    "\n",
    "pipe = make_pipeline(preprocessor, clf)\n",
    "\n",
    "print(\"Training RandomForest pipeline ...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c82bb423-5b62-40cf-baea-ca95a600bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: models\\loan_model_rf_baseline_v1_20251011T083235Z.joblib\n",
      "MODEL_VERSION = 'rf_baseline_v1_20251011T083235Z'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAM\\AppData\\Local\\Temp\\ipykernel_21084\\473293729.py:10: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  MODEL_VERSION = f\"rf_baseline_v1_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}\"\n"
     ]
    }
   ],
   "source": [
    "# Save model artifact (self-contained)\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# model pipeline 'pipe' must already exist in this kernel (trained)\n",
    "# If pipe is not defined, re-run the training cell first.\n",
    "\n",
    "# Create a timestamped model version and safe file path\n",
    "MODEL_VERSION = f\"rf_baseline_v1_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "MODELS_DIR = \"models\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, f\"loan_model_{MODEL_VERSION}.joblib\")\n",
    "\n",
    "# Save\n",
    "joblib.dump(pipe, MODEL_FILE)\n",
    "print(f\"Saved model to: {MODEL_FILE}\")\n",
    "print(f\"MODEL_VERSION = '{MODEL_VERSION}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe42b178-17f4-4890-b4e1-39a5ed1d7809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation (test):\n",
      "Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000, AUC: 1.0000\n",
      "Confusion matrix:\n",
      "[[102   0]\n",
      " [  0  98]]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate on test set\n",
    "# -------------------------\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_proba = pipe.predict_proba(X_test)[:,1] if hasattr(pipe, \"predict_proba\") else pipe.decision_function(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test))>1 else np.nan\n",
    "\n",
    "print(\"Evaluation (test):\")\n",
    "print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdadac70-fcb6-44fe-acd3-2465f262d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared predictions for 1000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAM\\AppData\\Local\\Temp\\ipykernel_21084\\1965800682.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  preds['ModelRunDate'] = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Prepare predictions for all data (or a selection)\n",
    "# -------------------------\n",
    "# Predict probabilities for all rows in df_model (you could limit to recent loans)\n",
    "X_all = df_model[features]\n",
    "probs_all = pipe.predict_proba(X_all)[:,1] if hasattr(pipe, \"predict_proba\") else pipe.decision_function(X_all)\n",
    "labels_all = (probs_all >= 0.5).astype(int)  # threshold 0.5 — tune as needed\n",
    "\n",
    "preds = df_model[['ApplicationID', 'CustomerID', 'ApplicationDate']].copy()\n",
    "preds['Pred_Prob'] = probs_all\n",
    "preds['Pred_Label'] = labels_all\n",
    "preds['ModelVersion'] = MODEL_VERSION\n",
    "preds['ModelRunDate'] = datetime.utcnow()\n",
    "\n",
    "print(\"Prepared predictions for\", len(preds), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46ccffd6-cba0-4e12-84f2-dc9db97e3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions to staging: dbo.LoanPredictions_Staging\n"
     ]
    }
   ],
   "source": [
    "# Minimal: define variable first (recommended)\n",
    "STAGING_TABLE = 'dbo.LoanPredictions_Staging'   # <- define this before using it\n",
    "\n",
    "create_staging_sql = f\"\"\"\n",
    "IF OBJECT_ID('{STAGING_TABLE}') IS NULL\n",
    "BEGIN\n",
    "    CREATE TABLE {STAGING_TABLE} (\n",
    "        ApplicationID INT,\n",
    "        CustomerID INT,\n",
    "        ApplicationDate DATE,\n",
    "        Pred_Prob FLOAT,\n",
    "        Pred_Label BIT,\n",
    "        ModelVersion VARCHAR(100),\n",
    "        ModelRunDate DATETIME2\n",
    "    );\n",
    "END;\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_staging_sql))\n",
    "    # Clear staging\n",
    "    conn.execute(text(f\"DELETE FROM {STAGING_TABLE};\"))\n",
    "\n",
    "# Pandas to_sql: note use fast_executemany via the engine created earlier\n",
    "preds.to_sql(name=STAGING_TABLE.split('.')[-1], schema=STAGING_TABLE.split('.')[0],\n",
    "             con=engine, if_exists='append', index=False, method=None)\n",
    "print(\"Wrote predictions to staging:\", STAGING_TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f562f68d-8934-494a-a636-bd817a61b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE completed into dbo.LoanPredictions\n"
     ]
    }
   ],
   "source": [
    "# Step 9: MERGE into target LoanPredictions table\n",
    "# -------------------------\n",
    "TARGET_TABLE = globals().get('TARGET_TABLE', 'dbo.LoanPredictions')\n",
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {TARGET_TABLE} AS Target\n",
    "USING {STAGING_TABLE} AS Src\n",
    "  ON Target.ApplicationID = Src.ApplicationID AND Target.ModelVersion = Src.ModelVersion\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    Target.Pred_Prob = Src.Pred_Prob,\n",
    "    Target.Pred_Label = Src.Pred_Label,\n",
    "    Target.ModelRunDate = Src.ModelRunDate\n",
    "WHEN NOT MATCHED BY TARGET THEN\n",
    "  INSERT (ApplicationID, CustomerID, ApplicationDate, Pred_Prob, Pred_Label, ModelVersion, ModelRunDate)\n",
    "  VALUES (Src.ApplicationID, Src.CustomerID, Src.ApplicationDate, Src.Pred_Prob, Src.Pred_Label, Src.ModelVersion, Src.ModelRunDate);\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(merge_sql))\n",
    "print(\"MERGE completed into\", TARGET_TABLE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10b86f16-1167-4a16-9648-655a339cce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top LoanPredictions rows:\n",
      "   PredictionID  ApplicationID  CustomerID ApplicationDate  Pred_Prob  \\\n",
      "0            11           2011        1118      2024-06-13   1.000000   \n",
      "1            10           2010        1135      2022-04-21   0.020000   \n",
      "2             9           2009        1289      2023-10-02   0.993333   \n",
      "3             8           2008        1319      2023-01-21   0.980000   \n",
      "4             7           2007        1257      2022-04-29   0.026667   \n",
      "5             6           2006        1458      2023-09-10   0.993333   \n",
      "6             5           2005        1406      2024-02-09   1.000000   \n",
      "7             4           2004        1380      2024-10-05   1.000000   \n",
      "8             3           2003        1333      2023-05-19   0.993333   \n",
      "9             2           2002        1124      2024-09-18   0.993333   \n",
      "\n",
      "   Pred_Label                     ModelVersion               ModelRunDate  \n",
      "0        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "1       False  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "2        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "3        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "4       False  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "5        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "6        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "7        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "8        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "9        True  rf_baseline_v1_20251011T083235Z 2025-10-11 08:34:31.550733  \n",
      "Pipeline finished. Model version: rf_baseline_v1_20251011T083235Z\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Quick verification from DB\n",
    "# -------------------------\n",
    "with engine.connect() as conn:\n",
    "    res = pd.read_sql(\"SELECT TOP (10) * FROM dbo.LoanPredictions ORDER BY ModelRunDate DESC;\", conn)\n",
    "    print(\"Top LoanPredictions rows:\")\n",
    "    print(res.head(10))\n",
    "\n",
    "print(\"Pipeline finished. Model version:\", MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93efe08-063d-4215-8c4b-faf499d9e89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
